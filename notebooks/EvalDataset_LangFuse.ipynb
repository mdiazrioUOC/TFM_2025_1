{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the evaluation Dataset with Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 20:26:07,934 - phenopy - INFO - checking if config file exists: /Users/malenadiazrio/.phenopy\n",
      "2025-05-06 20:26:07,934 - phenopy - INFO - phenopy 0.6.0\n",
      "2025-05-06 20:26:07,937 - phenopy - INFO - Using configuration file: /Users/malenadiazrio/.phenopy/phenopy.ini\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client, evaluate\n",
    "from phenopy.score import Scorer\n",
    "from langsmith.schemas import Run, Example\n",
    "from phenopy.build_hpo import generate_annotated_hpo_network\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../utils')\n",
    "\n",
    "import customchain as cc\n",
    "custom_chain = cc.custom_chain\n",
    "\n",
    "import rawgptchain as rgc\n",
    "rawgptchain = rgc.rawgptchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cc)\n",
    "custom_chain = cc.custom_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de los datasets de RAG-HPO y GSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id='cm9x6ivv80096pf0604is8yzj', name='RAGHPO', description='Dataset en español para la evaluación de herramientas de codificación fenotípica.', metadata={'date': '2025-04-25', 'type': 'benchmark', 'author': 'mdiazrio'}, project_id='cm9vlvsif0006pf07xcychmbe', created_at=datetime.datetime(2025, 4, 25, 19, 23, 13, 748000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 4, 25, 19, 23, 13, 748000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset(\n",
    "    name=\"RAGHPO\",\n",
    "    # optional description\n",
    "    description=\"Dataset en español para la evaluación de herramientas de codificación fenotípica.\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"author\": \"mdiazrio\",\n",
    "        \"date\": \"2025-04-25\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/RAG-HPO/Test_Cases.csv')\n",
    "df[\"annotations\"] = df.annotations.apply(eval)\n",
    "df = df.rename(columns={'esp':'clinical_note'})\n",
    "input_keys = ['clinical_note']\n",
    "output_keys = ['annotations'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_DIR=\"../../resources\"\n",
    "with open(os.path.join(RESOURCES_DIR, \"hpo_es.json\"), \"r\") as fp:\n",
    "    hpo = json.load(fp)\n",
    "valid_ids = [x['id'] for x in hpo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output):\n",
    "    hpo_list = [s.strip() for s in output]\n",
    "    hpo_list = [j for j in hpo_list if re.compile(r\"^HP:\\d{7}$\").match(j)]\n",
    "    hpo_list = [i for i in hpo_list if i in valid_ids]\n",
    "    return {\"annotations\":hpo_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows(): \n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=\"RAGHPO\",\n",
    "        input={\"clinical_note\": row['clinical_note']},\n",
    "        expected_output=process_output(row['annotations'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GSCESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id='cm9xwlj4n015vpf06u8x2xkjn', name='GSCESP', description='Dataset en español para la evaluación de herramientas de codificación fenotípica.', metadata={'date': '2025-04-26', 'type': 'benchmark', 'author': 'mdiazrio'}, project_id='cm9vlvsif0006pf07xcychmbe', created_at=datetime.datetime(2025, 4, 26, 7, 33, 7, 223000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 4, 26, 7, 33, 7, 223000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset(\n",
    "    name=\"GSCESP\",\n",
    "    # optional description\n",
    "    description=\"Dataset en español para la evaluación de herramientas de codificación fenotípica.\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"author\": \"mdiazrio\",\n",
    "        \"date\": \"2025-04-26\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "annotations = []\n",
    "for file in os.listdir(\"../../datasets/GCS+_ESP/Text\"):\n",
    "    with open(os.path.join(\"../../datasets/GCS+_ESP/Text\", file), \"r\") as fp:\n",
    "        texts.append(fp.read())\n",
    "    annots = pd.read_csv(os.path.join(\"../../datasets/GSC+/Annotations\", file), header=None, sep=\"\\t\")\n",
    "    annots[1] = annots[1].apply(lambda x: x.split(\"|\")[0].strip())\n",
    "    annotations.append(annots[1].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_esp = pd.DataFrame({\"esp\":texts, \"annotations\":annotations})\n",
    "gcs_esp.rename(columns={\"esp\":\"clinical_note\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output):\n",
    "    hpo_list = [s.strip() for s in output]\n",
    "    hpo_list = [s.replace('_', ':') for s in hpo_list]\n",
    "    hpo_list = [j for j in hpo_list if re.compile(r\"^HP:\\d{7}$\").match(j)]\n",
    "    hpo_list = [i for i in hpo_list if i in valid_ids]\n",
    "    hpo_list = list(set(hpo_list))\n",
    "    return {\"annotations\":hpo_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in gcs_esp[0:5].iterrows(): \n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=\"Pruebas\",\n",
    "        input={\"clinical_note\": row['clinical_note']},\n",
    "        expected_output=process_output(row['annotations'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_hpo_code(text):\n",
    "    # Check for full HPO code\n",
    "    match = re.search(r'HP:\\d{7}', text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    \n",
    "    # Check for 7-digit number\n",
    "    match = re.search(r'\\b\\d{7}\\b', text)\n",
    "    if match:\n",
    "        return f\"HP:{match.group()}\"\n",
    "    \n",
    "    # Nothing found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_final_answer(outputs):\n",
    "    try:\n",
    "        clean_codes = [code.hpo_code.strip() for code in outputs[\"final_answer\"] if getattr(code, \"hpo_code\", None) is not None]\n",
    "        clean_codes = [extract_hpo_code(code) for code in clean_codes]\n",
    "        clean_codes = [code for code in clean_codes if code is not None]\n",
    "        return clean_codes\n",
    "    except:\n",
    "        return outputs[\"final_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenopy_data_directory = \"../../resources/\"\n",
    "\n",
    "# files used in building the annotated HPO network\n",
    "obo_file = os.path.join(phenopy_data_directory, 'hp.obo')\n",
    "disease_to_phenotype_file = os.path.join(phenopy_data_directory, 'phenotype.hpoa')\n",
    "\n",
    "hpo_network, alt2prim, disease_records = \\\n",
    "    generate_annotated_hpo_network(obo_file,\n",
    "                                   disease_to_phenotype_file)\n",
    "\n",
    "scorer = Scorer(hpo_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can still pass in Run and Example objects if we'd like\n",
    "def traditional_metrics(outputs: dict, reference_outputs: dict) -> list[dict]:\n",
    "    \"\"\"Check precision, recall and f1.\"\"\"\n",
    "    predicted_terms = clean_final_answer(outputs)\n",
    "    real_terms = reference_outputs[\"annotations\"]\n",
    "    precision = 0 if len(predicted_terms) == 0 else sum([int(term in real_terms) for term in predicted_terms]) / len(predicted_terms)\n",
    "    recall = 0 if len(real_terms) == 0 else sum([int(term in predicted_terms) for term in real_terms]) / len(real_terms)\n",
    "    f1 = 0 if (precision + recall) == 0 else round(2 * (precision * recall) / (precision + recall),2)\n",
    "\n",
    "    return [\n",
    "        {\"key\": \"precision\", \"score\": precision},\n",
    "        {\"key\": \"recall\", \"score\": recall},\n",
    "        {\"key\": \"f1\", \"score\": f1},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check semantic similarity using phenopy.\"\"\"\n",
    "    predicted_terms = clean_final_answer(outputs)\n",
    "    real_terms = reference_outputs[\"annotations\"]\n",
    "    try:\n",
    "        score = scorer.score_term_sets_basic(predicted_terms, real_terms)\n",
    "    except:\n",
    "        score = -1\n",
    "    return [{\"key\": \"semantic similarity\", \"score\":score }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check Jaccard similarity between two sets.\"\"\"\n",
    "    predicted_terms = set(clean_final_answer(outputs))\n",
    "    real_terms = set(reference_outputs[\"annotations\"])   \n",
    "    intersection = predicted_terms.intersection(real_terms)\n",
    "    union = predicted_terms.union(real_terms)\n",
    "    if not union:\n",
    "        return 1.0  # define similarity as 1.0 when both are empty\n",
    "    return [{\"key\": \"jaccard_similarity\", \"score\": len(intersection) / len(union)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_accuracy(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check retriever accuracy and recall.\"\"\"\n",
    "    predicted_candidates = set().union(*outputs[\"docs\"])\n",
    "    real_terms = set(reference_outputs[\"annotations\"])\n",
    "    recall = len(real_terms & predicted_candidates) / len(real_terms)           # = 2/2 = 1.0\n",
    "    precision = len(real_terms & predicted_candidates) / len(predicted_candidates)   \n",
    "\n",
    "    return [{\"key\": \"r_precision\", \"score\": precision}, \n",
    "            {\"key\": \"r_recall\", \"score\": recall}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cc)\n",
    "custom_chain = cc.custom_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item: division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "MAX_WORKERS = 5\n",
    "dataset = langfuse.get_dataset(\"GSCESP\")\n",
    "evaluators = [traditional_metrics, semantic_similarity, jaccard_similarity, retrieve_accuracy] \n",
    "run_name = \"NoHybrid-o4mini\"\n",
    "\n",
    "def process_item(item):\n",
    "    try:\n",
    "        handler = item.get_langchain_handler(run_name=run_name)\n",
    "        response = custom_chain.with_config({ \"callbacks\": [handler]}).invoke(item.input)\n",
    "        for evaluator in evaluators:\n",
    "            scores = evaluator(response, item.expected_output)\n",
    "            for score in scores:\n",
    "                langfuse.score(trace_id=handler.get_trace_id(), name=score[\"key\"], value=score[\"score\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item: {e}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    executor.map(process_item, dataset.items[100:])\n",
    "\n",
    "# Ensure all data is sent\n",
    "langfuse.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
