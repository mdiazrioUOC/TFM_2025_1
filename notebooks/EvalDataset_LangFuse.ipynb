{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the evaluation Dataset with Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 17:53:15,623 - phenopy - INFO - checking if config file exists: /Users/malenadiazrio/.phenopy\n",
      "2025-05-25 17:53:15,624 - phenopy - INFO - phenopy 0.6.0\n",
      "2025-05-25 17:53:15,624 - phenopy - INFO - Using configuration file: /Users/malenadiazrio/.phenopy/phenopy.ini\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client, evaluate\n",
    "from phenopy.score import Scorer\n",
    "from langsmith.schemas import Run, Example\n",
    "from phenopy.build_hpo import generate_annotated_hpo_network\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "load_dotenv()\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['PROJECT_DIR'], 'src'))\n",
    "\n",
    "import utils.customchain as cc\n",
    "custom_chain = cc.custom_chain\n",
    "\n",
    "import utils.rawgptchain as rgc\n",
    "rawgptchain = rgc.rawgptchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cc)\n",
    "custom_chain = cc.custom_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de los datasets de RAG-HPO y GSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id='cm9x6ivv80096pf0604is8yzj', name='RAGHPO', description='Dataset en español para la evaluación de herramientas de codificación fenotípica.', metadata={'date': '2025-04-25', 'type': 'benchmark', 'author': 'mdiazrio'}, project_id='cm9vlvsif0006pf07xcychmbe', created_at=datetime.datetime(2025, 4, 25, 19, 23, 13, 748000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 5, 25, 13, 3, 28, 726000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset(\n",
    "    name=\"RAGHPO\",\n",
    "    # optional description\n",
    "    description=\"Dataset en español para la evaluación de herramientas de codificación fenotípica.\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"author\": \"mdiazrio\",\n",
    "        \"date\": \"2025-04-25\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/RAG-HPO/Test_Cases.csv')\n",
    "df[\"annotations\"] = df.annotations.apply(eval)\n",
    "df = df.rename(columns={'esp':'clinical_note'})\n",
    "input_keys = ['clinical_note']\n",
    "output_keys = ['annotations'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_DIR=\"../../resources\"\n",
    "with open(os.path.join(RESOURCES_DIR, \"hpo_es.json\"), \"r\") as fp:\n",
    "    hpo = json.load(fp)\n",
    "valid_ids = [x['id'] for x in hpo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output):\n",
    "    hpo_list = [s.strip() for s in output]\n",
    "    hpo_list = [j for j in hpo_list if re.compile(r\"^HP:\\d{7}$\").match(j)]\n",
    "    hpo_list = [i for i in hpo_list if i in valid_ids]\n",
    "    return {\"annotations\":hpo_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows(): \n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=\"RAGHPO\",\n",
    "        input={\"clinical_note\": row['clinical_note']},\n",
    "        expected_output=process_output(row['annotations'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GSCESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id='cm9xwlj4n015vpf06u8x2xkjn', name='GSCESP', description='Dataset en español para la evaluación de herramientas de codificación fenotípica.', metadata={'date': '2025-04-26', 'type': 'benchmark', 'author': 'mdiazrio'}, project_id='cm9vlvsif0006pf07xcychmbe', created_at=datetime.datetime(2025, 4, 26, 7, 33, 7, 223000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 4, 26, 7, 33, 7, 223000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfuse.create_dataset(\n",
    "    name=\"GSCESP\",\n",
    "    # optional description\n",
    "    description=\"Dataset en español para la evaluación de herramientas de codificación fenotípica.\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"author\": \"mdiazrio\",\n",
    "        \"date\": \"2025-04-26\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR=os.path.join(os.environ['PROJECT_DIR'],\"datasets/GSC+\")\n",
    "texts = []\n",
    "annotations = []\n",
    "for file in os.listdir(DATASET_DIR + \"_ESP/Text\"):\n",
    "    with open(os.path.join(DATASET_DIR + \"_ESP/Text\", file), \"r\") as fp:\n",
    "        texts.append(fp.read())\n",
    "    annots = pd.read_csv(os.path.join(DATASET_DIR + \"/Annotations\", file), header=None, sep=\"\\t\")\n",
    "    annots[1] = annots[1].apply(lambda x: x.split(\"|\")[0].strip())\n",
    "    annotations.append(annots[1].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_esp = pd.DataFrame({\"esp\":texts, \"annotations\":annotations})\n",
    "gcs_esp.rename(columns={\"esp\":\"clinical_note\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.162280701754385"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcs_esp.annotations.apply(lambda x: len(x)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/resources/Voyage Embeddings/embeddings_w_synonyms.pkl\", \"rb\") as fp:\n",
    "    embeddings = pkl.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output):\n",
    "    hpo_list = [s.strip() for s in output]\n",
    "    hpo_list = [s.replace('_', ':') for s in hpo_list]\n",
    "    hpo_list = [j for j in hpo_list if re.compile(r\"^HP:\\d{7}$\").match(j)]\n",
    "    hpo_list = [i for i in hpo_list if i in valid_ids]\n",
    "    hpo_list = list(set(hpo_list))\n",
    "    return {\"annotations\":hpo_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in gcs_esp[0:5].iterrows(): \n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=\"Pruebas\",\n",
    "        input={\"clinical_note\": row['clinical_note']},\n",
    "        expected_output=process_output(row['annotations'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_hpo_code(text):\n",
    "    # Check for full HPO code\n",
    "    match = re.search(r'HP:\\d{7}', text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    \n",
    "    # Check for 7-digit number\n",
    "    match = re.search(r'\\d{7}', text)\n",
    "    if match:\n",
    "        return f\"HP:{match.group()}\"\n",
    "    \n",
    "    # Nothing found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_final_answer(outputs):\n",
    "    try:\n",
    "        clean_codes = [code.hpo_code.strip() for code in outputs[\"final_answer\"] if getattr(code, \"hpo_code\", None) is not None]\n",
    "        clean_codes = [extract_hpo_code(code) for code in clean_codes]\n",
    "        clean_codes = [code for code in clean_codes if code is not None]\n",
    "        return set(clean_codes)\n",
    "    except:\n",
    "        return outputs[\"final_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenopy_data_directory = os.path.join(os.environ[\"PROJECT_DIR\"],\"./resources/\")\n",
    "\n",
    "# files used in building the annotated HPO network\n",
    "obo_file = os.path.join(phenopy_data_directory, 'hp.obo')\n",
    "disease_to_phenotype_file = os.path.join(phenopy_data_directory, 'phenotype.hpoa')\n",
    "\n",
    "hpo_network, alt2prim, disease_records = \\\n",
    "    generate_annotated_hpo_network(obo_file,\n",
    "                                   disease_to_phenotype_file)\n",
    "\n",
    "scorer = Scorer(hpo_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can still pass in Run and Example objects if we'd like\n",
    "def traditional_metrics(outputs: dict, reference_outputs: dict) -> list[dict]:\n",
    "    \"\"\"Check precision, recall and f1.\"\"\"\n",
    "    predicted_terms = clean_final_answer(outputs)\n",
    "    real_terms = reference_outputs[\"annotations\"]\n",
    "    precision = 0 if len(predicted_terms) == 0 else sum([int(term in real_terms) for term in predicted_terms]) / len(predicted_terms)\n",
    "    recall = 0 if len(real_terms) == 0 else sum([int(term in predicted_terms) for term in real_terms]) / len(real_terms)\n",
    "    f1 = 0 if (precision + recall) == 0 else round(2 * (precision * recall) / (precision + recall),2)\n",
    "\n",
    "    return [\n",
    "        {\"key\": \"precision\", \"score\": precision},\n",
    "        {\"key\": \"recall\", \"score\": recall},\n",
    "        {\"key\": \"f1\", \"score\": f1},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check semantic similarity using phenopy.\"\"\"\n",
    "    predicted_terms = clean_final_answer(outputs)\n",
    "    real_terms = reference_outputs[\"annotations\"]\n",
    "    try:\n",
    "        score = scorer.score_term_sets_basic(predicted_terms, real_terms)\n",
    "    except:\n",
    "        score = -1\n",
    "    return [{\"key\": \"semantic similarity\", \"score\":score }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check Jaccard similarity between two sets.\"\"\"\n",
    "    predicted_terms = set(clean_final_answer(outputs))\n",
    "    real_terms = set(reference_outputs[\"annotations\"])   \n",
    "    intersection = predicted_terms.intersection(real_terms)\n",
    "    union = predicted_terms.union(real_terms)\n",
    "    if not union:\n",
    "        return 1.0  # define similarity as 1.0 when both are empty\n",
    "    return [{\"key\": \"jaccard_similarity\", \"score\": len(intersection) / len(union)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_accuracy(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check retriever accuracy and recall.\"\"\"\n",
    "    predicted_candidates = set().union(*outputs[\"docs\"])\n",
    "    real_terms = set(reference_outputs[\"annotations\"])\n",
    "    recall = len(real_terms & predicted_candidates) / len(real_terms)           # = 2/2 = 1.0\n",
    "    precision = len(real_terms & predicted_candidates) / len(predicted_candidates)   \n",
    "\n",
    "    return [{\"key\": \"r_precision\", \"score\": precision}, \n",
    "            {\"key\": \"r_recall\", \"score\": recall}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cc)\n",
    "custom_chain = cc.custom_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item: status_code: 500, body: {'message': 'Internal Server Error', 'error': 'An unknown error occurred'}\n",
      "Error processing item: status_code: 500, body: {'message': 'Internal Server Error', 'error': 'An unknown error occurred'}\n",
      "Error processing item: status_code: 500, body: {'message': 'Internal Server Error', 'error': 'An unknown error occurred'}\n",
      "Error processing item: status_code: 500, body: {'message': 'Internal Server Error', 'error': 'An unknown error occurred'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item: division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "MAX_WORKERS = 5\n",
    "dataset = langfuse.get_dataset(\"GSCESP\")\n",
    "evaluators = [traditional_metrics, semantic_similarity, jaccard_similarity, retrieve_accuracy] \n",
    "run_name = \"top10\"\n",
    "\n",
    "def process_item(item):\n",
    "    try:\n",
    "        handler = item.get_langchain_handler(run_name=run_name)\n",
    "        response = custom_chain.with_config({ \"callbacks\": [handler]}).invoke(item.input)\n",
    "        for evaluator in evaluators:\n",
    "            scores = evaluator(response, item.expected_output)\n",
    "            for score in scores:\n",
    "                langfuse.score(trace_id=handler.get_trace_id(), name=score[\"key\"], value=score[\"score\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item: {e}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    executor.map(process_item, dataset.items)\n",
    "\n",
    "# for item in dataset.items[0:20]:\n",
    "#     process_item(item)\n",
    "\n",
    "\n",
    "# Ensure all data is sent\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to recompute scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = langfuse.get_dataset(\"RAGHPO\")\n",
    "expected_responses = {}\n",
    "for item in dataset.items:\n",
    "    expected_responses[item.id] = item.expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_final_answer(outputs):\n",
    "    # try:\n",
    "    clean_codes = [code[\"hpo_code\"].strip() for code in outputs[\"final_answer\"] if  code[\"hpo_code\"] is not None]\n",
    "    clean_codes = [extract_hpo_code(code) for code in clean_codes]\n",
    "    clean_codes = [code for code in clean_codes if code is not None]\n",
    "    return set(clean_codes)\n",
    "    # except:\n",
    "    #     return outputs[\"final_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langfuse_run_ids(dataset_name, dataset_run_name):\n",
    "    dataset_run = langfuse.get_dataset_run(\n",
    "    dataset_name=dataset_name, dataset_run_name=dataset_run_name\n",
    "    )\n",
    "    trace_ids = [run_item.trace_id for run_item in dataset_run.dataset_run_items]\n",
    "    return trace_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ids = langfuse_run_ids(\"RAGHPO\", \"top10\") #\"2025-04-27_11-43-07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_score(trace_id):\n",
    "    trace = langfuse.fetch_trace(trace_id).data\n",
    "    evaluators = [traditional_metrics, semantic_similarity, jaccard_similarity, retrieve_accuracy] \n",
    "    scores = []\n",
    "    for evaluator in evaluators:\n",
    "        scores += evaluator(trace.output,expected_responses[trace.metadata['dataset_item_id']])\n",
    "\n",
    "    scores = {score['key']:score['score'] for score in scores}\n",
    "    for score in trace.scores:\n",
    "        if score.name in scores:\n",
    "            langfuse.score(\n",
    "                id = score.id,\n",
    "                name = score.name,\n",
    "                value= scores[score.name],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 5\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    executor.map(recompute_score, trace_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
