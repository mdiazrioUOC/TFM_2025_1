{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traducir y crear los datasets de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import deepl\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "DEEPL_API_KEY = os.getenv(\"DEEPL_API_KEY\")\n",
    "translator = deepl.Translator(DEEPL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducción del dataset GSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = \"../../datasets/GSC+/Annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_hpo_codes = set()\n",
    "for file in os.listdir(annotations_folder)[1:]:\n",
    "    df = pd.read_csv(os.path.join(annotations_folder, file), sep=r\"[\\t|]\", header=None, names=[\"positions\", \"hpo_code\", \"hpo_name\"], engine='python')\n",
    "    evaluation_hpo_codes.update(df.hpo_code.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_folder = \"../../datasets/GSC+/Text\"\n",
    "text_content = {}\n",
    "total_chars = 0\n",
    "for file_name in os.listdir(text_folder):\n",
    "     with open(os.path.join(text_folder, file_name), \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "            content = file.read()\n",
    "            total_chars += len(content)\n",
    "            text_content[file_name] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el número de ids en chunks de 50\n",
    "def chunk_dict_keys(d, chunk_size=50):\n",
    "    values = list(d.keys())  \n",
    "    return [values[i:i + chunk_size] for i in range(0, len(values), chunk_size)]\n",
    "\n",
    "chunks = chunk_dict_keys(text_content, chunk_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_to_json(chunks, filename=\"chunks.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(chunks, file, indent=4)\n",
    "\n",
    "def load_chunks_from_json(filename=\"chunks.json\"):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        chunks = json.load(file)  # Load JSON into a Python list\n",
    "    return chunks\n",
    "\n",
    "save_chunks_to_json(chunks, \"../../resources/evalGCS+_chunks.json\")\n",
    "chunks = load_chunks_from_json(\"../../resources/evalGCS+_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../../datasets/GCS+_ESP/Text\"):\n",
    "    os.makedirs(\"../../datasets/GCS+_ESP/Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There eval dataset has 226191 characters in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"There eval dataset has {total_chars} characters in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(ids, traducciones, dir):\n",
    "    for id, traduccion in zip(ids, traducciones):\n",
    "        with open(os.path.join(dir, id), \"w\") as fp:\n",
    "            fp.write(traduccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for _ in tqdm(range(1), desc=\"Traduciendo\"):\n",
    "    processed_codes = os.listdir(\"../../datasets/GCS+_ESP/Text\")\n",
    "    for i in range(len(chunks)):\n",
    "        if all([s in processed_codes for s in chunks[i]]):\n",
    "            continue\n",
    "        break\n",
    "    if i == len(chunks):\n",
    "        break\n",
    "    texto_original = [text_content[j] for j in chunks[i]]\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "\n",
    "    save_to_txt(chunks[i], texto_traduccion, \"../../datasets/GCS+_ESP/Text\")  # Guarda en \"output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compruebo que se han traducido todos los archivos\n",
    "og_files = os.listdir(\"../../datasets/GSC+/Text\")\n",
    "esp_files = os.listdir(\"../../datasets/GCS+_ESP/Text\")\n",
    "set(og_files) - set(esp_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traducción del dataset RAG-HPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../datasets/Test_Cases.xlsx\")\n",
    "df.columns  = [\"id\", \"eng\", \"esp\"]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Documentos no traducidos: {df[df.esp.isna()].shape[0]}\") \n",
    "print(f\"Caracteres a traducir: {sum(df[df.esp.isna()].eng.apply(lambda x: len(x)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "while df[df.esp.isna()].shape[0] > 0:\n",
    "    texto_original = df[df.esp.isna()].iloc[0:10].eng.to_list()\n",
    "    idxs = df[df.esp.isna()].iloc[0:10].index\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "    df.loc[idxs, \"esp\"] = texto_traduccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id = df.id.apply(lambda x: int(x[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../datasets/RAG-HPO/Test_Cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 4)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = pd.read_csv(\"../../datasets/RAG-HPO/Test_Cases.csv\")\n",
    "eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura y asociación de los textos con sus anotaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../datasets/RAG-HPO/RAG-HPO_Tests_and_Data_Analysis.xlsx\", header=None)\n",
    "df = df[[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = {}\n",
    "id = None\n",
    "read = False\n",
    "for i, row in df.iterrows():\n",
    "    if pd.isna(row[2]):\n",
    "        read=False\n",
    "    if row[1] == \"Manually Assigned HPO Terms\":\n",
    "        if id is not None:\n",
    "            annotations[id] = annot_list\n",
    "        id = None\n",
    "        annot_list = []\n",
    "        read=True\n",
    "        if not pd.isna(row[0]) and row[0]!=\"Case\":\n",
    "            id = row[0] + 96\n",
    "\n",
    "    if read and row[1] == \"Phenotype name\" and id is None:\n",
    "        if row[0] == \"Case\":\n",
    "            id = df.loc[i+1, 0]\n",
    "        else:\n",
    "            id = row[0]\n",
    "\n",
    "    if read and isinstance(row[2], str) and row[2] not in ['none', 'HPO ID']:\n",
    "        annot_list.append(row[2])\n",
    "annotations[id] = annot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../datasets/RAG-HPO/Test_Cases.csv\")\n",
    "test.drop(columns = \"Unnamed: 0\", inplace=True)\n",
    "test[\"annotations\"] = test.id.apply(lambda x: annotations[x])\n",
    "test.to_csv(\"../../datasets/RAG-HPO/Test_Cases.csv\", index=False)\n",
    "test.annotations = test.annotations.apply(eval)\n",
    "test.annotations = test.annotations.apply(lambda x: [j.strip() for j in x])\n",
    "test.annotations = test.annotations.apply(lambda x: [j for j in x if re.compile(r\"^HP:\\d{7}$\").match(j)])\n",
    "test.to_csv(\"../../datasets/RAG-HPO/Test_Cases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura y asociación de términos del GSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "annotations = []\n",
    "for file in os.listdir(\"../../datasets/GCS+_ESP/Text\"):\n",
    "    with open(os.path.join(\"../../datasets/GCS+_ESP/Text\", file), \"r\") as fp:\n",
    "        texts.append(fp.read())\n",
    "    annots = pd.read_csv(os.path.join(\"../../datasets/GSC+/Annotations\", file), header=None, sep=\"\\t\")\n",
    "    annots[1] = annots[1].apply(lambda x: x.split(\"|\")[0].strip())\n",
    "    annotations.append(annots[1].to_list())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
