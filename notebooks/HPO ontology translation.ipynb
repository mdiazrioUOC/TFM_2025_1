{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO ontology translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import deepl\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "DEEPL_API_KEY = os.getenv(\"DEEPL_API_KEY\")\n",
    "translator = deepl.Translator(DEEPL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving ontology by API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_request(url, file_name):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Archivo '{file_name}' descargado con éxito.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.json().get('message', 'Error desconocido')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def github_download_request(url, file_name):\n",
    "    # Hacer la solicitud GET\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        file_data = response.json()\n",
    "        download_url = file_data[\"download_url\"]  # URL de descarga del archivo\n",
    "        # Descargar el archivo\n",
    "        file_response = requests.get(download_url)\n",
    "\n",
    "        if file_response.status_code == 200:\n",
    "            with open(file_name, \"wb\") as file:\n",
    "                file.write(file_response.content)\n",
    "            print(f\"Archivo '{file_name}' descargado con éxito.\")\n",
    "        else:\n",
    "            print(\"Error al descargar el archivo.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.json().get('message', 'Error desconocido')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archivo de traducción oficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository configuration\n",
    "esp_file_path = \"src/translations/hp-es-preprocessed.babelon.tsv\"  \n",
    "RESOURCE_FOLDER = \"../../resources\"\n",
    "\n",
    "url = f\"https://api.github.com/repos/obophenotype/human-phenotype-ontology/contents/{esp_file_path}\"\n",
    "esp_file_name = os.path.join(RESOURCE_FOLDER, esp_file_path.split(\"/\")[-1])\n",
    "\n",
    "# github_download_request(url, esp_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archivo en inglés con descripciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_FOLDER = \"../../resources\"\n",
    "\n",
    "oboa_file_path = \"hp/phenotype.hpoa\"\n",
    "url = f\"http://purl.obolibrary.org/obo/{oboa_file_path}\"\n",
    "eng_file_name = os.path.join(RESOURCE_FOLDER, \"phenotype.hpoa\")\n",
    "\n",
    "# download_request(url, eng_file_name)\n",
    "\n",
    "obo_file_path = \"hp.obo\"\n",
    "\n",
    "url = f\"http://purl.obolibrary.org/obo/{obo_file_path}\"\n",
    "eng_file_name = os.path.join(RESOURCE_FOLDER, \"hp.obo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir archivo .obo a diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_dict(line):\n",
    "    new_dict = {}\n",
    "    for element in line:\n",
    "        if \": \" in element:\n",
    "            k,v = element.split(\": \", 1)\n",
    "            if k in new_dict:\n",
    "                if isinstance(new_dict[k], str):\n",
    "                    new_dict[k] = [new_dict[k]]\n",
    "                new_dict[k].append(v)\n",
    "            else:\n",
    "                new_dict[k] = v\n",
    "    return new_dict\n",
    "\n",
    "def find_translation(id, translation_df, col=\"translation_value\"):\n",
    "    if id in translation_df.index:\n",
    "        return {\"esp_name\": translation_df.loc[id, col]}\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el archivo de traducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OFFICIAL     17533\n",
       "CANDIDATE     1428\n",
       "Name: translation_status, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esp_translation = pd.read_csv(esp_file_name, sep=\"\\t\")\n",
    "esp_translation.set_index(\"subject_id\", inplace=True)\n",
    "esp_translation.translation_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el archivo de la ontología"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total HPO terms: 19533\n"
     ]
    }
   ],
   "source": [
    "with open(eng_file_name, \"r\") as fp:\n",
    "    lines = fp.read()\n",
    "lines = re.split(r\"(\\[Term\\]|\\[Typedef\\])\", lines)\n",
    "lines = [lines[i+1] for i in range(len(lines)) if lines[i] == '[Term]']\n",
    "lines = [line.split(\"\\n\") for line in lines] #separar cada propiedad\n",
    "lines = [str_to_dict(line) for line in lines] #convertir cada elemento en un diccionario\n",
    "print(f\"Total HPO terms: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-obsolete HPO terms: 19077\n"
     ]
    }
   ],
   "source": [
    "lines = [line for line in lines if not \"is_obsolete\" in line]\n",
    "print(f\"Total non-obsolete HPO terms: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir la traducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'HP:0000001',\n",
       " 'name': 'All',\n",
       " 'comment': 'Root of all terms in the Human Phenotype Ontology.',\n",
       " 'xref': 'UMLS:C0444868',\n",
       " 'esp_name': 'Todos'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = [line.update(find_translation(line[\"id\"], esp_translation)) for line in lines]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total HPO terms with spanish translation: 18506\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total HPO terms with spanish translation: {len([line for line in lines if 'esp_name' in line])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3184"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_mapped_names = [s for s in lines if \"esp_name\" not in s]\n",
    "sum([len(name) for name in not_mapped_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_synonym(s):\n",
    "        match = re.search(r'\"(.*)\"(.*)', s)\n",
    "        if match:\n",
    "            s= match.group(1)\n",
    "        return s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synonyms(synonyms):\n",
    "    if isinstance(synonyms, list):\n",
    "        for i, s in enumerate(synonyms):\n",
    "            synonyms[i] = clean_synonym(s)\n",
    "    else:\n",
    "        synonyms = clean_synonym(synonyms)\n",
    "    return {\"synonym\": synonyms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'HP:0000006',\n",
       " 'name': 'Autosomal dominant inheritance',\n",
       " 'alt_id': ['HP:0001415',\n",
       "  'HP:0001447',\n",
       "  'HP:0001448',\n",
       "  'HP:0001451',\n",
       "  'HP:0001452',\n",
       "  'HP:0001455',\n",
       "  'HP:0001456',\n",
       "  'HP:0001463'],\n",
       " 'def': '\"A mode of inheritance that is observed for traits related to a gene encoded on one of the autosomes (i.e., the human chromosomes 1-22) in which a trait manifests in heterozygotes. In the context of medical genetics, an autosomal dominant disorder is caused when a single copy of the mutant allele is present. Males and females are affected equally, and can both transmit the disorder with a risk of 50% for each child of inheriting the mutant allele.\" [https://orcid.org/0000-0002-0736-9199]',\n",
       " 'synonym': ['Autosomal dominant',\n",
       "  'Autosomal dominant form',\n",
       "  'Autosomal dominant type',\n",
       "  'monoallelic_autosomal'],\n",
       " 'xref': ['SNOMEDCT_US:263681008', 'UMLS:C0443147'],\n",
       " 'is_a': 'HP:0034345 ! Mendelian inheritance',\n",
       " 'esp_name': 'Herencia autosómica dominante'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = [line.update(process_synonyms(line[\"synonym\"])) for line in lines if \"synonym\" in line]\n",
    "lines[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synonyms(synonyms):\n",
    "    if isinstance(synonyms, list):\n",
    "        total_length = 0\n",
    "        for i, s in enumerate(synonyms):\n",
    "            total_length += len(s)\n",
    "    else:\n",
    "        total_length = len(synonyms)\n",
    "    return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670352"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([process_synonyms(line[\"synonym\"]) for line in lines if \"synonym\" in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_definitions(defi):\n",
    "    defi = re.sub(r'\\[http.*?\\]'  , '', defi)\n",
    "    defi = re.search(r'\"(.*)\"', defi).group(1)\n",
    "    defi = defi.strip(' ')\n",
    "    return defi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = {line[\"id\"]:process_definitions(line[\"def\"]) for line in lines if \"def\" in line}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16504"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(definitions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el número de ids en chunks de 50\n",
    "def chunk_dict_keys(d, chunk_size=50):\n",
    "    values = list(d.keys())  \n",
    "    return [values[i:i + chunk_size] for i in range(0, len(values), chunk_size)]\n",
    "\n",
    "# chunks = chunk_dict_keys(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar el resultado en un archivo para asegurar que el split siempre sea igual\n",
    "import json\n",
    "\n",
    "def save_chunks_to_json(chunks, filename=\"chunks.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(chunks, file, indent=4)\n",
    "\n",
    "def load_chunks_from_json(filename=\"chunks.json\"):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        chunks = json.load(file)  # Load JSON into a Python list\n",
    "    return chunks\n",
    "\n",
    "# save_chunks_to_json(chunks, \"../../resources/def_chunks.json\")\n",
    "# chunks = load_chunks_from_json(\"../../resources/def_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3079"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traducción con DeepL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(lista1, lista2, filename=\"output.txt\"):\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as file:  # \"a\" = append mode\n",
    "        for val1, val2 in zip(lista1, lista2):\n",
    "            file.write(f\"{val1}\\t{val2}\\n\")  # Tab-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traduciendo:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[38;5;28mlen\u001b[39m(chunks):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m texto_original = \u001b[43m[\u001b[49m\u001b[43mdefinitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m idioma_destino = \u001b[33m\"\u001b[39m\u001b[33mES\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Código de idioma (ES = español, EN = inglés, etc.)\u001b[39;00m\n\u001b[32m     13\u001b[39m traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[38;5;28mlen\u001b[39m(chunks):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m texto_original = [\u001b[43mdefinitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m chunks[i]]\n\u001b[32m     11\u001b[39m idioma_destino = \u001b[33m\"\u001b[39m\u001b[33mES\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Código de idioma (ES = español, EN = inglés, etc.)\u001b[39;00m\n\u001b[32m     13\u001b[39m traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(50), desc=\"Traduciendo\"):\n",
    "    df = pd.read_csv(\"../../resources/traduccion_definiciones.txt\", delimiter=\"\\t\", header=None)  \n",
    "    processed_codes = df.iloc[:, 0].tolist()\n",
    "    for i in range(len(chunks)):\n",
    "        if all([s in processed_codes for s in chunks[i]]):\n",
    "            continue\n",
    "        break\n",
    "    if i == len(chunks):\n",
    "        break\n",
    "    texto_original = [definitions[j] for j in chunks[i]]\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "\n",
    "    save_to_txt(chunks[i], texto_traduccion, \"../../resources/traduccion_definiciones.txt\")  # Guarda en \"output.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociar las definiciones en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_definitions = pd.read_csv(\"../../resources/traduccion_definiciones.txt\", delimiter=\"\\t\", header=None)  \n",
    "esp_definitions.columns=[\"code\", \"definition\"]\n",
    "esp_definitions.set_index(\"code\", inplace=True)\n",
    "\n",
    "_ = [hpo.update({\"esp_def\":esp_definitions.loc[hpo[\"id\"], \"definition\"]}) for hpo in lines if hpo[\"id\"] in esp_definitions.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traducción de nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19026"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = {i[\"id\"]:i[\"name\"] for i in lines if \"esp_name\" not in i}\n",
    "sum([len(s) for s in names.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = chunk_dict_keys(names)\n",
    "# save_chunks_to_json(chunks, \"../../resources/name_chunks.json\")\n",
    "chunks = load_chunks_from_json(\"../../resources/name_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVO_TRADUCCION_NOMBRES = \"../../resources/traduccion_nombres.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traduciendo: 100%|██████████| 1/1 [00:00<00:00, 243.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heyy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(1), desc=\"Traduciendo\"):\n",
    "    if os.path.exists(ARCHIVO_TRADUCCION_NOMBRES):\n",
    "        df = pd.read_csv(ARCHIVO_TRADUCCION_NOMBRES, delimiter=\"\\t\", header=None)\n",
    "        processed_codes = df.iloc[:, 0].tolist()\n",
    "        for i in range(len(chunks)):\n",
    "            if all([s in processed_codes for s in chunks[i]]):\n",
    "                continue\n",
    "            break\n",
    "    else:\n",
    "        i=0\n",
    "\n",
    "    if i == len(chunks):\n",
    "        break\n",
    "    texto_original = [names[j] for j in chunks[i]]\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "\n",
    "    save_to_txt(chunks[i], texto_traduccion, ARCHIVO_TRADUCCION_NOMBRES)  # Guarda en \"output.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociar con los nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_names = pd.read_csv(ARCHIVO_TRADUCCION_NOMBRES, delimiter=\"\\t\", header=None)  \n",
    "esp_names.columns=[\"code\", \"name\"]\n",
    "esp_names.set_index(\"code\", inplace=True)\n",
    "\n",
    "_ = [hpo.update({\"esp_name\":esp_names.loc[hpo[\"id\"], \"name\"]}) for hpo in lines if hpo[\"id\"] in esp_names.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traducción de sinónimos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Términos con un solo sinónimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_synonym = {line['id']: line[\"synonym\"] for line in lines if \"synonym\" in line if isinstance(line[\"synonym\"], str)}\n",
    "chunks = chunk_dict_keys(single_synonym, chunk_size=100)\n",
    "# save_chunks_to_json(chunks, \"../../resources/synonym_chunks.json\")\n",
    "chunks = load_chunks_from_json(\"../../resources/synonym_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVO_TRADUCCION_SINONIMOS = \"../../resources/traduccion_sinonimos.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traduciendo: 100%|██████████| 15/15 [00:06<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(15), desc=\"Traduciendo\"):\n",
    "    if os.path.exists(ARCHIVO_TRADUCCION_SINONIMOS):\n",
    "        df = pd.read_csv(ARCHIVO_TRADUCCION_SINONIMOS, delimiter=\"\\t\", header=None)\n",
    "        processed_codes = df.iloc[:, 0].tolist()\n",
    "        for i in range(len(chunks)):\n",
    "            if all([s in processed_codes for s in chunks[i]]):\n",
    "                continue\n",
    "            break\n",
    "    else:\n",
    "        i=0\n",
    "\n",
    "    if i == len(chunks):\n",
    "        break\n",
    "    texto_original = [single_synonym[j] for j in chunks[i]]\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "\n",
    "    save_to_txt(chunks[i], texto_traduccion, ARCHIVO_TRADUCCION_SINONIMOS)  # Guarda en \"output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10859"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([line for line in lines if \"synonym\" in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Términos con más de un sinónimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5272"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_synonym = {line['id']: line[\"synonym\"] for line in lines if \"synonym\" in line if isinstance(line[\"synonym\"], list)}\n",
    "len(multiple_synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_dict_keys(multiple_synonym, chunk_size=100)\n",
    "# save_chunks_to_json(chunks, \"../../resources/mult_synonym_chunks.json\")\n",
    "chunks = load_chunks_from_json(\"../../resources/mult_synonym_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt2(lista1, lista2, filename=\"output.txt\"):\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as file:  # \"a\" = append mode\n",
    "        for hpo_code, positions in lista1.items():\n",
    "            file.write(f\"{hpo_code}\\t{lista2[positions[0]: positions[1]]}\\n\")  # Tab-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traduciendo: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(1), desc=\"Traduciendo\"):\n",
    "    if os.path.exists(ARCHIVO_TRADUCCION_SINONIMOS):\n",
    "        df = pd.read_csv(ARCHIVO_TRADUCCION_SINONIMOS, delimiter=\"\\t\", header=None)\n",
    "        processed_codes = df.iloc[:, 0].tolist()\n",
    "        for i in range(len(chunks)):\n",
    "            if all([s in processed_codes for s in chunks[i]]):\n",
    "                continue\n",
    "            break\n",
    "    else:\n",
    "        i=0\n",
    "\n",
    "    if i == len(chunks):\n",
    "        break\n",
    "    list_pos = 0 \n",
    "    texto_original = []\n",
    "    posiciones_originales = {}\n",
    "    for j in chunks[i]:\n",
    "        for synonym in multiple_synonym[j]:\n",
    "            texto_original.append(synonym)\n",
    "        posiciones_originales[j] = [list_pos, list_pos + len(multiple_synonym[j])]\n",
    "        list_pos += len(multiple_synonym[j])\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "    \n",
    "    save_to_txt2(posiciones_originales, texto_traduccion, ARCHIVO_TRADUCCION_SINONIMOS)  # Guarda en \"output.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociar con sinónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10859, 2)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(ARCHIVO_TRADUCCION_SINONIMOS, delimiter=\"\\t\", header=None)  \n",
    "df.drop_duplicates(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(synonyms):\n",
    "    try: \n",
    "        return eval(synonyms)\n",
    "    except:\n",
    "        return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_synonyms = pd.read_csv(ARCHIVO_TRADUCCION_SINONIMOS, delimiter=\"\\t\", header=None)  \n",
    "esp_synonyms.columns=[\"code\", \"synonyms\"]\n",
    "esp_synonyms.set_index(\"code\", inplace=True)\n",
    "\n",
    "_ = [hpo.update({\"esp_synonyms\":convert_to_list(esp_synonyms.loc[hpo[\"id\"], \"synonyms\"])})\n",
    "                 for hpo in lines if hpo[\"id\"] in esp_synonyms.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19077"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_addons = pd.read_csv(os.path.join(\"../../resources/\", \"HPO_addons.csv\"))\n",
    "hpo_addons = hpo_addons['info'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_dict_keys(hpo_addons, chunk_size=400)\n",
    "save_chunks_to_json(chunks, \"../../resources/addons_chunks.json\")\n",
    "chunks = load_chunks_from_json(\"../../resources/addons_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVO_TRADUCCION_ADDONS = \"../../resources/traduccion_addons.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traduciendo: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(2), desc=\"Traduciendo\"):\n",
    "    if os.path.exists(ARCHIVO_TRADUCCION_ADDONS):\n",
    "        df = pd.read_csv(ARCHIVO_TRADUCCION_ADDONS, delimiter=\"\\t\", header=None)\n",
    "        processed_codes = df.iloc[:, 0].tolist()\n",
    "        for i in range(len(chunks)):\n",
    "            if all([s in processed_codes for s in chunks[i]]):\n",
    "                continue\n",
    "            break\n",
    "    else:\n",
    "        i=0\n",
    "\n",
    "    if i == len(chunks):\n",
    "        break\n",
    "    texto_original = [hpo_addons[j] for j in chunks[i]]\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "\n",
    "    save_to_txt(chunks[i], texto_traduccion, ARCHIVO_TRADUCCION_ADDONS)  # Guarda en \"output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "addons = pd.read_csv(ARCHIVO_TRADUCCION_ADDONS, delimiter=\"\\t\", header=None)  \n",
    "addons.columns = [\"HPO_ID\", \"description\"]\n",
    "# df[[\"HPO_ID\", 1]].to_csv(\"../../resources/traduccion_addons.txt\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "addons = addons.groupby(\"HPO_ID\").agg({'description':list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [hpo.update({\"esp_addons\":addons.loc[hpo[\"id\"], \"description\"]})\n",
    "                 for hpo in lines if hpo[\"id\"] in addons.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar el diccionario creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../resources/hpo_es.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(lines, fp, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
