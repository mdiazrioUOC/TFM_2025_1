{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the evaluation Dataset with Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import json\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client, evaluate\n",
    "from phenopy.score import Scorer\n",
    "from langsmith.schemas import Run, Example\n",
    "from phenopy.build_hpo import generate_annotated_hpo_network\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "import customchain as cc\n",
    "custom_chain = cc.custom_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cc)\n",
    "custom_chain = cc.custom_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de los datasets de RAG-HPO y GSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/RAG-HPO/Test_Cases.csv')\n",
    "df[\"annotations\"] = df.annotations.apply(eval)\n",
    "df = df.rename(columns={'esp':'clinical_note'})\n",
    "input_keys = ['clinical_note']\n",
    "output_keys = ['annotations'] \n",
    "\n",
    "dataset = client.upload_dataframe(\n",
    "    df=df,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"RAG-HPO eval dataset\",\n",
    "    description=\"Dataset en español para la evaluación de herramientas de codificación fenotípica.\",\n",
    "    data_type=\"kv\" # The default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_DIR=\"../../resources\"\n",
    "with open(os.path.join(RESOURCES_DIR, \"hpo_es.json\"), \"r\") as fp:\n",
    "    hpo = json.load(fp)\n",
    "valid_ids = [x['id'] for x in hpo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output):\n",
    "    hpo_list = output[\"annotations\"]\n",
    "    hpo_list = [s.strip() for s in hpo_list]\n",
    "    hpo_list = [j for j in hpo_list if re.compile(r\"^HP:\\d{7}$\").match(j)]\n",
    "    hpo_list = [i for i in hpo_list if i in valid_ids]\n",
    "    return {\"annotations\":hpo_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = client.list_examples(dataset_name=\"RAG-HPO eval dataset\")\n",
    "ids = []\n",
    "metadata = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "for example in examples:\n",
    "    ids.append(example.id)\n",
    "    inputs.append(example.inputs)\n",
    "    outputs.append(process_output(example.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.update_examples(\n",
    "    dataset_name=\"RAG-HPO eval dataset\",\n",
    "    example_ids = ids,\n",
    "    inputs = inputs, \n",
    "    outputs = outputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "annotations = []\n",
    "for file in os.listdir(\"../../datasets/GCS+_ESP/Text\"):\n",
    "    with open(os.path.join(\"../../datasets/GCS+_ESP/Text\", file), \"r\") as fp:\n",
    "        texts.append(fp.read())\n",
    "    annots = pd.read_csv(os.path.join(\"../../datasets/GSC+/Annotations\", file), header=None, sep=\"\\t\")\n",
    "    annots[1] = annots[1].apply(lambda x: x.split(\"|\")[0].strip())\n",
    "    annotations.append(annots[1].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_esp = pd.DataFrame({\"esp\":texts, \"annotations\":annotations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys = ['esp']\n",
    "output_keys = ['annotations'] \n",
    "\n",
    "dataset = client.upload_dataframe(\n",
    "    df=gcs_esp,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"GSC eval dataset\",\n",
    "    description=\"Dataset en español para la evaluación de herramientas de codificación fenotípica.\",\n",
    "    data_type=\"kv\" # The default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_final_answer(outputs):\n",
    "    return [code.hpo_code.strip() for code in outputs[\"final answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenopy_data_directory = \"../../resources/\"\n",
    "\n",
    "# files used in building the annotated HPO network\n",
    "obo_file = os.path.join(phenopy_data_directory, 'hp.obo')\n",
    "disease_to_phenotype_file = os.path.join(phenopy_data_directory, 'phenotype.hpoa')\n",
    "\n",
    "hpo_network, alt2prim, disease_records = \\\n",
    "    generate_annotated_hpo_network(obo_file,\n",
    "                                   disease_to_phenotype_file)\n",
    "\n",
    "scorer = Scorer(hpo_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can still pass in Run and Example objects if we'd like\n",
    "def traditional_metrics(outputs: dict, reference_outputs: dict) -> list[dict]:\n",
    "    \"\"\"Check precision, recall and f1.\"\"\"\n",
    "    predicted_terms = clean_final_answer(outputs)\n",
    "    real_terms = eval(reference_outputs[\"annotations\"])\n",
    "    precision = 0 if len(predicted_terms) == 0 else sum([int(term in real_terms) for term in predicted_terms]) / len(predicted_terms)\n",
    "    recall = 0 if len(real_terms) == 0 else sum([int(term in predicted_terms) for term in real_terms]) / len(real_terms)\n",
    "    f1 = 0 if (precision + recall) == 0 else round(2 * (precision * recall) / (precision + recall),2)\n",
    "\n",
    "    return [\n",
    "        {\"key\": \"precision\", \"score\": precision},\n",
    "        {\"key\": \"recall\", \"score\": recall},\n",
    "        {\"key\": \"f1\", \"score\": f1},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check semantic similarity using phenopy.\"\"\"\n",
    "    predicted_terms = clean_final_answer(outputs)\n",
    "    real_terms = eval(reference_outputs[\"annotations\"])\n",
    "\n",
    "    return {\"key\": \"semantic similarity\", \"score\": scorer.score_term_sets_basic(predicted_terms, real_terms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score_term_sets_basic([\"HP:0033349\"], [\"HP:0001317\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check Jaccard similarity between two sets.\"\"\"\n",
    "    predicted_terms = set(clean_final_answer(outputs))\n",
    "    real_terms = set(eval(reference_outputs[\"annotations\"]))    \n",
    "    intersection = predicted_terms.intersection(real_terms)\n",
    "    union = predicted_terms.union(real_terms)\n",
    "    if not union:\n",
    "        return 1.0  # define similarity as 1.0 when both are empty\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_accuracy(outputs: dict, reference_outputs: dict)->float:\n",
    "    \"\"\"Check retriever accuracy and recall.\"\"\"\n",
    "    predicted_candidates = set().union(*outputs[\"docs\"])\n",
    "    real_terms = set(eval(reference_outputs[\"annotations\"]))  \n",
    "    recall = len(real_terms & predicted_candidates) / len(real_terms)           # = 2/2 = 1.0\n",
    "    precision = len(real_terms & predicted_candidates) / len(predicted_candidates)   \n",
    "\n",
    "    return [{\"key\": \"r_precision\", \"score\": precision}, \n",
    "            {\"key\": \"r_recall\", \"score\": recall}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(\n",
    "    custom_chain,\n",
    "    data=\"RAG-HPO eval dataset\",\n",
    "    evaluators=[traditional_metrics, semantic_similarity, jaccard_similarity, retrieve_accuracy]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
