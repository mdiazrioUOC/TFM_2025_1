{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO ontology loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malenadiazrio/Documents/UOC/TFM/TFM_2025_1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import tqdm \n",
    "import json\n",
    "import chromadb\n",
    "import voyageai\n",
    "import pickle as pkl\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import process, fuzz\n",
    "from langchain_chroma import Chroma\n",
    "from rapidfuzz.utils import default_process\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_voyageai import VoyageAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "PROJECT_DIR=os.environ[\"PROJECT_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PROJECT_DIR, \"resources\", \"hpo_es.json\"), \"r\") as fp:\n",
    "    hpo = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the desired fields of the ontology\n",
    "fields = [\"esp_name\", \"esp_def\", 'esp_synonyms', \"esp_addons\"] # \"is_a\",\n",
    "hpo_dict = {}\n",
    "\n",
    "for element in hpo:\n",
    "    hpo_dict[element[\"id\"]] = {field:element[field] for field in fields if field in element}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 19077\n",
      "Total elements with a spanish name: 19077\n",
      "Total elements with a spanish definition: 16504\n",
      "Total elements with a spanish synonym: 10852\n",
      "Total elements with a spanish addon: 1764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def_count = 0\n",
    "name_count = 0\n",
    "synonym_count = 0\n",
    "addon_count = 0\n",
    "for k,v in hpo_dict.items():\n",
    "    if \"esp_def\" in v:\n",
    "        def_count += 1\n",
    "    if \"esp_name\" in v:\n",
    "        name_count += 1\n",
    "    if \"esp_synonyms\" in v:\n",
    "        synonym_count += 1\n",
    "    if \"esp_addons\" in v:\n",
    "        addon_count += 1\n",
    "\n",
    "print(f\"\"\"Total docs: {len(hpo_dict)}\n",
    "Total elements with a spanish name: {name_count}\n",
    "Total elements with a spanish definition: {def_count}\n",
    "Total elements with a spanish synonym: {synonym_count}\n",
    "Total elements with a spanish addon: {addon_count}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lineage(s):\n",
    "    return s.split('!')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean lineage\n",
    "for k,v in hpo_dict.items():\n",
    "    if \"is_a\" in v:\n",
    "        if isinstance(v[\"is_a\"], list):\n",
    "            for i, parent in enumerate(v[\"is_a\"]):\n",
    "                v[\"is_a\"][i] = clean_lineage(parent)\n",
    "        else:\n",
    "            v[\"is_a\"] = clean_lineage(v[\"is_a\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean lineage\n",
    "def find_parent(hpo_code, hpo_dict=hpo_dict):\n",
    "    lineage = hpo_dict[hpo_code][\"is_a\"]\n",
    "    if isinstance(lineage, list):\n",
    "        parents = set(lineage)\n",
    "        for parent in lineage:\n",
    "            parents.update(find_parent(parent))\n",
    "        return parents\n",
    "    \n",
    "    if \"is_a\" not in hpo_dict[lineage]:\n",
    "        return []\n",
    "    \n",
    "    return [lineage] + list(find_parent(lineage))\n",
    "    \n",
    "\n",
    "_ = {v.update({\"lineage\": find_parent(k)}) for k,v in hpo_dict.items() if \"is_a\" in v}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_names_dict(terms, hpo_code, names_dict):\n",
    "    for term in terms:\n",
    "        term = term.lower()\n",
    "        if term in names_dict and hpo_code not in names_dict[term]:\n",
    "            names_dict[term] += [hpo_code]\n",
    "        else:\n",
    "            names_dict[term] = [hpo_code]\n",
    "    return names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_text = []\n",
    "metadata_list = []\n",
    "names_dict = {}\n",
    "for hpo_code, hpo_values in hpo_dict.items():\n",
    "    metadata = {\"hpo_id\":hpo_code}\n",
    "    cleaned_info = []\n",
    "    if \"esp_name\" in hpo_values:\n",
    "        cleaned_info.append(hpo_values[\"esp_name\"])\n",
    "        names_dict = add_to_names_dict([hpo_values[\"esp_name\"]], hpo_code, names_dict)\n",
    "    if \"esp_synonyms\" in hpo_values:\n",
    "        syn_list = hpo_values[\"esp_synonyms\"] if isinstance (hpo_values[\"esp_synonyms\"], list) else [hpo_values[\"esp_synonyms\"]]\n",
    "        syn_list = [str(s) for s in syn_list]\n",
    "        cleaned_info += syn_list\n",
    "        names_dict = add_to_names_dict(syn_list, hpo_code, names_dict)\n",
    "    if \"esp_def\" in hpo_values:\n",
    "        cleaned_info.append(hpo_values[\"esp_def\"])\n",
    "    cleaned_info = [str(s) for s in cleaned_info]\n",
    "    cleaned_info = [s.strip() + \".\" if not s.strip().endswith(\".\") else s.strip() for s in cleaned_info]\n",
    "    cleaned_info = \" \".join(cleaned_info)\n",
    "    documents_text.append(cleaned_info)\n",
    "    if \"lineage\" in hpo_values:\n",
    "        metadata[\"lineage\"] = \"->\".join(hpo_values[\"lineage\"])\n",
    "    metadata_list.append(metadata)\n",
    "ids_list = [v['hpo_id'] for v in metadata_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Voyage Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"voyage-3\"\n",
    "embeddings_model = VoyageAIEmbeddings(voyage_api_key=VOYAGE_API_KEY,model=\"voyage-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_delete = [i for i,doc in enumerate(documents_text) if not isinstance(doc,str)][0]\n",
    "documents_text.pop(idx_to_delete)\n",
    "metadata.pop(idx_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 23/23 [00:30<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "vo = voyageai.Client(api_key=VOYAGE_API_KEY)\n",
    "batch_size = 1000\n",
    "embeddings= []\n",
    "for i in tqdm.tqdm(range(len(embeddings), len(documents_text), batch_size), desc=\"Batch: \" ):       \n",
    "    response= vo.embed(\n",
    "        documents_text[i:i + batch_size], model=MODEL_NAME, input_type=\"document\"\n",
    "    )\n",
    "    embeddings += response.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar los resultados \n",
    "with open( os.path.join(PROJECT_DIR, \"./resources/Voyage Embeddings/embeddings_w_synonyms.pkl\"), \"wb\") as fp:\n",
    "    pkl.dump(embeddings, fp)\n",
    "\n",
    "with open( os.path.join(PROJECT_DIR, \"./resources/Voyage Embeddings/docs_w_synonyms.pkl\"), \"wb\") as fp:\n",
    "    pkl.dump(documents_text, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar los resultados\n",
    "with open(f\"{PROJECT_DIR}/resources/Voyage Embeddings/embeddings_w_synonyms.pkl\", \"rb\") as fp:\n",
    "    embeddings = pkl.load(fp)\n",
    "with open(f\"{PROJECT_DIR}/resources/Voyage Embeddings/docs_w_synonyms.pkl\", \"rb\") as fp:\n",
    "    docs = pkl.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into chroma client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(host='localhost', port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_client = chromadb.PersistentClient(path=\"../../chroma_db/Voyage3\")\n",
    "collection = chroma_client.get_or_create_collection(\"hpo_ontology_esp_FULL\")\n",
    "ids_list = [metadata[i]['hpo_id'] + str(i) for i in list(range(len(embeddings)))]\n",
    "BATCH_SIZE = 1000\n",
    "for i in range(0, len(embeddings), BATCH_SIZE):\n",
    "        collection.add(\n",
    "                embeddings=embeddings[i: i+BATCH_SIZE],\n",
    "                documents=documents_text[i: i+BATCH_SIZE],\n",
    "                metadatas=metadata[i: i+BATCH_SIZE],\n",
    "                ids = ids_list[i: i+BATCH_SIZE]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chroma = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=\"hpo_ontology_esp_FULL\",\n",
    "    embedding_function=embeddings_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19077 documents in the collection\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", langchain_chroma._collection.count(), \"documents in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='HP:0100542', metadata={'hpo_id': 'HP:0100542', 'lineage': 'HP:0012210->HP:0000077->HP:0010935->HP:0000079->HP:0000119->HP:0000118'}, page_content='Localización anormal de los riñones. Localización anormal de los riñones. Un lugar anormal del riñón.'),\n",
       " Document(id='HP:0008738', metadata={'hpo_id': 'HP:0008738', 'lineage': 'HP:0000075->HP:0000119->HP:0005217->HP:0001438->HP:0010935->HP:0000118->HP:0025031->HP:0012210->HP:0000077->HP:0000079'}, page_content='Riñón parcialmente duplicado. Riñón parcialmente duplicado. La presencia de un riñón parcialmente duplicado.'),\n",
       " Document(id='HP:0030157', metadata={'hpo_id': 'HP:0030157', 'lineage': 'HP:0012531->HP:0025142->HP:0000118'}, page_content='Dolor de costado. Dolor de costado. Dolor de riñón. Sensación desagradable caracterizada por molestias físicas (como pinchazos, palpitaciones o dolores) y que se percibe como originada en el flanco.'),\n",
       " Document(id='HP:0000085', metadata={'lineage': 'HP:0100542->HP:0012210->HP:0000077->HP:0010935->HP:0000079->HP:0000119->HP:0000118', 'hpo_id': 'HP:0000085'}, page_content='Riñón en herradura. Riñones fusionados. Riñón en herradura. Riñones en herradura. Conexión del riñón derecho y el izquierdo mediante un istmo de parénquima renal funcionante o tejido fibroso que atraviesa la línea media.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_chroma.as_retriever().invoke(\"Tiene dolor en el riñon izquierdo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into BM25 Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = []\n",
    "for id, metadata, page_content in zip(ids_list, metadata_list, documents_text):\n",
    "    docs_list.append(Document(id=id, metadata=metadata, page_content=page_content))\n",
    "\n",
    "keyword_retriever = BM25Retriever.from_documents(docs_list)\n",
    "keyword_retriever.invoke(\"convulsiones inducibles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../resources/keyword_retriever.pkl\", 'wb') as fp:\n",
    "    pkl.dump(keyword_retriever, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build search list: [(text, phenotype_id), ...]\n",
    "search_entries = []\n",
    "ids_list2 = []\n",
    "for id, docs in zip(ids_list, documents_text):\n",
    "    for doc in docs:\n",
    "        search_entries.append(doc)\n",
    "        ids_list2.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyRetriever:\n",
    "    search_entries = search_entries\n",
    "    ids_list = ids_list2\n",
    "\n",
    "    def invoke(self, query):\n",
    "        results = process.extract(query, search_entries,\n",
    "                                  scorer=fuzz.QRatio, limit=10,\n",
    "                                  processor=default_process)\n",
    "        return [(self.ids_list[result[2]], result[0], result[1]) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HP:0007332', 'Convulsiones hemifaciales.', 79.16666666666666),\n",
       " ('HP:0007332', 'Convulsiones hemifaciales.', 79.16666666666666),\n",
       " ('HP:0007359', 'Convulsiones focales.', 79.06976744186046),\n",
       " ('HP:0007359', 'Convulsiones focales.', 79.06976744186046),\n",
       " ('HP:0002373', 'Convulsiones febriles.', 77.27272727272727),\n",
       " ('HP:0002373', 'Convulsiones febriles.', 77.27272727272727),\n",
       " ('HP:0002373', 'Convulsiones inducidas por fiebre.', 75.0),\n",
       " ('HP:0033349', 'Convulsiones crecientes.', 73.91304347826086),\n",
       " ('HP:0010819', 'Convulsiones atónicas.', 72.72727272727273),\n",
       " ('HP:0033349', 'Convulsiones en serie.', 72.72727272727273)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzyretriever = FuzzyRetriever()\n",
    "fuzzyretriever.invoke(\"convulsiones inducibles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../resources/fuzzy_retriever.pkl\", 'wb') as fp:\n",
    "    pkl.dump({\"search_entries\": search_entries, \"ids\":ids_list2}, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
