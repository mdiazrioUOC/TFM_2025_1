{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO ontology translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import deepl\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "DEEPL_API_KEY = os.getenv(\"DEEPL_API_KEY\")\n",
    "translator = deepl.Translator(DEEPL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving ontology by API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_request(url, file_name):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Archivo '{file_name}' descargado con éxito.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.json().get('message', 'Error desconocido')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def github_download_request(url, file_name):\n",
    "    # Hacer la solicitud GET\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        file_data = response.json()\n",
    "        download_url = file_data[\"download_url\"]  # URL de descarga del archivo\n",
    "        # Descargar el archivo\n",
    "        file_response = requests.get(download_url)\n",
    "\n",
    "        if file_response.status_code == 200:\n",
    "            with open(file_name, \"wb\") as file:\n",
    "                file.write(file_response.content)\n",
    "            print(f\"Archivo '{file_name}' descargado con éxito.\")\n",
    "        else:\n",
    "            print(\"Error al descargar el archivo.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.json().get('message', 'Error desconocido')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archivo de traducción oficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository configuration\n",
    "esp_file_path = \"src/translations/hp-es-preprocessed.babelon.tsv\"  \n",
    "RESOURCE_FOLDER = \"../../resources\"\n",
    "\n",
    "url = f\"https://api.github.com/repos/obophenotype/human-phenotype-ontology/contents/{esp_file_path}\"\n",
    "esp_file_name = os.path.join(RESOURCE_FOLDER, esp_file_path.split(\"/\")[-1])\n",
    "\n",
    "# github_download_request(url, esp_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archivo en inglés con descripciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_FOLDER = \"../../resources\"\n",
    "obo_file_path = \"hp.obo\"\n",
    "\n",
    "url = f\"http://purl.obolibrary.org/obo/{obo_file_path}\"\n",
    "eng_file_name = os.path.join(RESOURCE_FOLDER, obo_file_path)\n",
    "\n",
    "# download_request(url, eng_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir archivo .obo a diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_dict(line):\n",
    "    new_dict = {}\n",
    "    for element in line:\n",
    "        if \": \" in element:\n",
    "            k,v = element.split(\": \", 1)\n",
    "            if k in new_dict:\n",
    "                if isinstance(new_dict[k], str):\n",
    "                    new_dict[k] = [new_dict[k]]\n",
    "                new_dict[k].append(v)\n",
    "            else:\n",
    "                new_dict[k] = v\n",
    "    return new_dict\n",
    "\n",
    "def find_translation(id, translation_df, col=\"translation_value\"):\n",
    "    if id in translation_df.index:\n",
    "        return {\"esp_name\": translation_df.loc[id, col]}\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el archivo de traducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translation_status\n",
       "OFFICIAL     17533\n",
       "CANDIDATE     1428\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esp_translation = pd.read_csv(esp_file_name, sep=\"\\t\")\n",
    "esp_translation.set_index(\"subject_id\", inplace=True)\n",
    "esp_translation.translation_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el archivo de la ontología"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total HPO terms: 19533\n"
     ]
    }
   ],
   "source": [
    "with open(eng_file_name, \"r\") as fp:\n",
    "    lines = fp.read()\n",
    "lines = re.split(r\"(\\[Term\\]|\\[Typedef\\])\", lines)\n",
    "lines = [lines[i+1] for i in range(len(lines)) if lines[i] == '[Term]']\n",
    "lines = [line.split(\"\\n\") for line in lines] #separar cada propiedad\n",
    "lines = [str_to_dict(line) for line in lines] #convertir cada elemento en un diccionario\n",
    "print(f\"Total HPO terms: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadir la traducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'HP:0000001',\n",
       " 'name': 'All',\n",
       " 'comment': 'Root of all terms in the Human Phenotype Ontology.',\n",
       " 'xref': 'UMLS:C0444868',\n",
       " 'esp_name': 'Todos'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = [line.update(find_translation(line[\"id\"], esp_translation)) for line in lines]\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3189"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_mapped_names = [s for s in lines if \"esp_name\" not in s]\n",
    "sum([len(name) for name in not_mapped_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_synonym(s):\n",
    "        match = re.search(r'\"(.*)\"(.*)', s)\n",
    "        if match:\n",
    "            s= match.group(1)\n",
    "        return s.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synonyms(synonyms):\n",
    "    if isinstance(synonyms, list):\n",
    "        for i, s in enumerate(synonyms):\n",
    "            synonyms[i] = clean_synonym(s)\n",
    "    else:\n",
    "        synonyms = clean_synonym(synonyms)\n",
    "    return {\"synonym\": synonyms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'HP:0000006',\n",
       " 'name': 'Autosomal dominant inheritance',\n",
       " 'alt_id': ['HP:0001415',\n",
       "  'HP:0001447',\n",
       "  'HP:0001448',\n",
       "  'HP:0001451',\n",
       "  'HP:0001452',\n",
       "  'HP:0001455',\n",
       "  'HP:0001456',\n",
       "  'HP:0001463'],\n",
       " 'def': '\"A mode of inheritance that is observed for traits related to a gene encoded on one of the autosomes (i.e., the human chromosomes 1-22) in which a trait manifests in heterozygotes. In the context of medical genetics, an autosomal dominant disorder is caused when a single copy of the mutant allele is present. Males and females are affected equally, and can both transmit the disorder with a risk of 50% for each child of inheriting the mutant allele.\" [https://orcid.org/0000-0002-0736-9199]',\n",
       " 'synonym': ['Autosomal dominant',\n",
       "  'Autosomal dominant form',\n",
       "  'Autosomal dominant type',\n",
       "  'monoallelic_autosomal'],\n",
       " 'xref': ['SNOMEDCT_US:263681008', 'UMLS:C0443147'],\n",
       " 'is_a': 'HP:0034345 ! Mendelian inheritance',\n",
       " 'esp_name': 'Herencia autosómica dominante'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = [line.update(process_synonyms(line[\"synonym\"])) for line in lines if \"synonym\" in line]\n",
    "lines[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synonyms(synonyms):\n",
    "    if isinstance(synonyms, list):\n",
    "        total_length = 0\n",
    "        for i, s in enumerate(synonyms):\n",
    "            total_length += len(s)\n",
    "    else:\n",
    "        total_length = len(synonyms)\n",
    "    return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670583"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([process_synonyms(line[\"synonym\"]) for line in lines if \"synonym\" in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_definitions(defi):\n",
    "    defi = re.sub(r'\\[http.*?\\]'  , '', defi)\n",
    "    defi = re.search(r'\"(.*)\"', defi).group(1)\n",
    "    defi = defi.strip(' ')\n",
    "    return defi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = {line[\"id\"]:process_definitions(line[\"def\"]) for line in lines if \"def\" in line}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16509"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(definitions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el número de ids en chunks de 50\n",
    "def chunk_dict_keys(d, chunk_size=50):\n",
    "    values = list(d.keys())  \n",
    "    return [values[i:i + chunk_size] for i in range(0, len(values), chunk_size)]\n",
    "\n",
    "chunks = chunk_dict_keys(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar el resultado en un archivo para asegurar que el split siempre sea igual\n",
    "import json\n",
    "\n",
    "def save_chunks_to_json(chunks, filename=\"chunks.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(chunks, file, indent=4)\n",
    "\n",
    "def load_chunks_from_json(filename=\"chunks.json\"):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        chunks = json.load(file)  # Load JSON into a Python list\n",
    "    return chunks\n",
    "\n",
    "# save_chunks_to_json(chunks, \"../../resources/def_chunks.json\")\n",
    "chunks = load_chunks_from_json(\"../../resources/def_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16509"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traducción con DeepL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(lista1, lista2, filename=\"output.txt\"):\n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as file:  # \"a\" = append mode\n",
    "        for val1, val2 in zip(lista1, lista2):\n",
    "            file.write(f\"{val1}\\t{val2}\\n\")  # Tab-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(50), desc=\"Traduciendo\"):\n",
    "    df = pd.read_csv(\"../../resources/traduccion_definiciones.txt\", delimiter=\"\\t\", header=None)  \n",
    "    processed_codes = df.iloc[:, 0].tolist()\n",
    "    for i in range(len(chunks)):\n",
    "        if all([s in processed_codes for s in chunks[i]]):\n",
    "            continue\n",
    "        break\n",
    "    if i == len(chunks):\n",
    "        break\n",
    "    texto_original = [definitions[j] for j in chunks[i]]\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "\n",
    "    save_to_txt(chunks[i], texto_traduccion, \"../../resources/traduccion_definiciones.txt\")  # Guarda en \"output.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociar las definiciones en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_definitions = pd.read_csv(\"../../resources/traduccion_definiciones.txt\", delimiter=\"\\t\", header=None)  \n",
    "esp_definitions.columns=[\"code\", \"definition\"]\n",
    "esp_definitions.set_index(\"code\", inplace=True)\n",
    "\n",
    "_ = [hpo.update({\"esp_def\":esp_definitions.loc[hpo[\"id\"], \"definition\"]}) for hpo in lines if hpo[\"id\"] in esp_definitions.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traducción de nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19065"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = {i[\"id\"]:i[\"name\"] for i in lines if \"esp_name\" not in i}\n",
    "sum([len(s) for s in names.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = chunk_dict_keys(names)\n",
    "# save_chunks_to_json(chunks, \"../../resources/name_chunks.json\")\n",
    "chunks = load_chunks_from_json(\"../../resources/name_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVO_TRADUCCION_NOMBRES = \"../../resources/traduccion_nombres.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traduciendo: 100%|██████████| 1/1 [00:00<00:00, 243.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heyy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(1), desc=\"Traduciendo\"):\n",
    "    if os.path.exists(ARCHIVO_TRADUCCION_NOMBRES):\n",
    "        df = pd.read_csv(ARCHIVO_TRADUCCION_NOMBRES, delimiter=\"\\t\", header=None)\n",
    "        processed_codes = df.iloc[:, 0].tolist()\n",
    "        for i in range(len(chunks)):\n",
    "            if all([s in processed_codes for s in chunks[i]]):\n",
    "                continue\n",
    "            break\n",
    "    else:\n",
    "        i=0\n",
    "\n",
    "    if i == len(chunks):\n",
    "        break\n",
    "    texto_original = [names[j] for j in chunks[i]]\n",
    "    idioma_destino = \"ES\"  # Código de idioma (ES = español, EN = inglés, etc.)\n",
    "\n",
    "    traduccion = translator.translate_text(texto_original, target_lang=idioma_destino)\n",
    "    texto_traduccion = [t.text for t in traduccion]\n",
    "\n",
    "    save_to_txt(chunks[i], texto_traduccion, ARCHIVO_TRADUCCION_NOMBRES)  # Guarda en \"output.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociar con los nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_names = pd.read_csv(ARCHIVO_TRADUCCION_NOMBRES, delimiter=\"\\t\", header=None)  \n",
    "esp_names.columns=[\"code\", \"name\"]\n",
    "esp_names.set_index(\"code\", inplace=True)\n",
    "\n",
    "_ = [hpo.update({\"esp_name\":esp_names.loc[hpo[\"id\"], \"name\"]}) for hpo in lines if hpo[\"id\"] in esp_names.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar el diccionario creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../resources/hpo_es.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(lines, fp, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
